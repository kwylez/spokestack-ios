<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Classes  Reference</title>
    <link rel="stylesheet" type="text/css" href="css/jazzy.css" />
    <link rel="stylesheet" type="text/css" href="css/highlight.css" />
    <meta charset='utf-8'>
    <script src="js/jquery.min.js" defer></script>
    <script src="js/jazzy.js" defer></script>
    
  </head>
  <body>
    <a name="//apple_ref/swift/Section/Classes" class="dashAnchor"></a>
    <a title="Classes  Reference"></a>
    <header>
      <div class="content-wrapper">
        <p><a href="index.html">Spokestack_iOS Docs</a> (89% documented)</p>
      </div>
    </header>
    <div class="content-wrapper">
      <p id="breadcrumbs">
        <a href="index.html">Spokestack_iOS Reference</a>
        <img id="carat" src="img/carat.png" />
        Classes  Reference
      </p>
    </div>
    <div class="content-wrapper">
      <nav class="sidebar">
        <ul class="nav-groups">
          <li class="nav-group-name">
            <a href="Classes.html">Classes</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Classes/AppleSpeechRecognizer.html">AppleSpeechRecognizer</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/AppleWakewordRecognizer.html">AppleWakewordRecognizer</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/CoreMLWakewordRecognizer.html">CoreMLWakewordRecognizer</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/SpeechConfiguration.html">SpeechConfiguration</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/SpeechContext.html">SpeechContext</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/SpeechPipeline.html">SpeechPipeline</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/TFLiteWakewordRecognizer.html">TFLiteWakewordRecognizer</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/WebRTCVAD.html">WebRTCVAD</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Enums.html">Enumerations</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Enums/AudioError.html">AudioError</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/SPSpeechRecognitionError.html">SPSpeechRecognitionError</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/SPSpeechRecognitionResult.html">SPSpeechRecognitionResult</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/SpeechPipelineError.html">SpeechPipelineError</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/SpeechProcessors.html">SpeechProcessors</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/VADDecision.html">VADDecision</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/VADError.html">VADError</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/VADMode.html">VADMode</a>
              </li>
              <li class="nav-group-task">
                <a href="Enums/WakewordModelError.html">WakewordModelError</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Protocols.html">Protocols</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Protocols/PipelineDelegate.html">PipelineDelegate</a>
              </li>
              <li class="nav-group-task">
                <a href="Protocols/SpeechEventListener.html">SpeechEventListener</a>
              </li>
              <li class="nav-group-task">
                <a href="Protocols/SpeechProcessor.html">SpeechProcessor</a>
              </li>
              <li class="nav-group-task">
                <a href="Protocols/VADDelegate.html">VADDelegate</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Structs.html">Structures</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Structs/SignalProcessing.html">SignalProcessing</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SignalProcessing/FFTWindowType.html">– FFTWindowType</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Trace.html">Trace</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Trace/Level.html">– Level</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Typealiases.html">Type Aliases</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Typealiases.html#/s:14Spokestack_iOS24SpeechRecognitionClosurea">SpeechRecognitionClosure</a>
              </li>
            </ul>
          </li>
        </ul>
      </nav>
      <article class="main-content">
        <section>
          <section class="section">
            <h1>Classes</h1>
            <p>The following classes are available globally.</p>

          </section>
          <section class="section task-group-section">
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer"></a>
                    <a name="//apple_ref/swift/Class/AppleSpeechRecognizer" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer">AppleSpeechRecognizer</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>This pipeline component uses the Apple <code>SFSpeech</code> API to stream audio samples for speech recognition.</p>

<p>When the speech pipeline coordination event is started via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer begins streaming buffered frames to the Apple ASR API for recognition. Once the speech pipeline coordination is stopped via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, or when the Apple ASR API indicates a completed speech event, the recognizer completes the API request and calls the <code><a href="Protocols/SpeechEventListener.html">SpeechEventListener</a></code> delegate&rsquo;s  <code>didRecognize</code> event  with the updated global speech context (including the audio transcript and confidence).</p>

                        <a href="Classes/AppleSpeechRecognizer.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">@objc</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="kt">AppleSpeechRecognizer</span> <span class="p">:</span> <span class="kt">NSObject</span><span class="p">,</span> <span class="kt"><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer"></a>
                    <a name="//apple_ref/swift/Class/AppleWakewordRecognizer" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer">AppleWakewordRecognizer</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>This pipeline component uses the Apple <code>SFSpeech</code> API to stream audio samples for wakeword recognition.</p>

<p>When the speech pipeline coordination event is started via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer begins streaming buffered frames to the Apple ASR API for recognition. Upon wakeword or wakephrase recognition, the pipeline activation event is triggered and the recognizer completes the API request and awaits another coordination event. When the speech pipeline coordination is stopped via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer completes the API request  and awaits another coordination event.</p>

                        <a href="Classes/AppleWakewordRecognizer.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">@objc</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="kt">AppleWakewordRecognizer</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer"></a>
                    <a name="//apple_ref/swift/Class/CoreMLWakewordRecognizer" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer">CoreMLWakewordRecognizer</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>This pipeline component streams audio samples and uses a CoreML classifier to detect keywords (i.e. [null], up, dog) and aggregates them into phrases (up dog) to process for wakeword recognition. Once a wakeword phrase is detected, the speech pipeline is activated. The speech pipeline remains active until the user stops talking or the activation timeout is reached.</p>

<p>When the speech pipeline coordination event is started via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer begins streaming buffered frames that are normalized and then converted to the magnitude Short-Time Fourier Transform (STFT) representation over a hopped sliding window. This linear spectrogram is then converted to a mel spectrogram via a <q>filter</q> Tensorflow model. These mel frames are batched together into a sliding window.</p>

<p>The mel spectrogram represents the features to be passed to the keyword classifier, which is implemented in a <q>detect</q> CoreML model. This classifier outputs posterior probabilities for each keyword (and a null output 0, which represents non-keyword speech).</p>

<p>The detector&rsquo;s outputs are considered noisy, so they are maintained in a sliding window and passed through a moving mean filter. The smoothed posteriors are then maintained in another sliding window for phrasing. The phraser attempts to match one of the configured keyword sequences using the maximum posterior for each word. If a sequence match is found, the speech pipeline is activated.</p>

<p>Upon the pipeline activation event, the recognizer completes processing and awaits another coordination event. When the speech pipeline coordination is stopped via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer stops processing and awaits another coordination event.</p>

                        <a href="Classes/CoreMLWakewordRecognizer.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">public</span> <span class="kd">class</span> <span class="kt">CoreMLWakewordRecognizer</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration"></a>
                    <a name="//apple_ref/swift/Class/SpeechConfiguration" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration">SpeechConfiguration</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>Configuration properties for the pipeline abstraction to pass down to implementations.</p>

                        <a href="Classes/SpeechConfiguration.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">@objc</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="kt">SpeechConfiguration</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)SpeechContext"></a>
                    <a name="//apple_ref/swift/Class/SpeechContext" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)SpeechContext">SpeechContext</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>This class maintains global state for the speech pipeline, allowing pipeline components to communicate information among themselves and event handlers.</p>

                        <a href="Classes/SpeechContext.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">@objc</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="kt">SpeechContext</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline"></a>
                    <a name="//apple_ref/swift/Class/SpeechPipeline" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline">SpeechPipeline</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>This is the primary client entry point to the SpokeStack framework. It dynamically binds to configured components that implement the pipeline interfaces for reading audio frames and performing speech recognition tasks.</p>

<p>The pipeline may be stopped/restarted any number of times during its lifecycle. While stopped, the pipeline consumes as few resources as possible. The pipeline runs asynchronously on a dedicated thread, so it does not block the caller when performing I/O and speech processing.</p>

<p>When running, the pipeline communicates with the client via delegates that receive events.</p>
<pre class="highlight swift"><code><span class="c1">// assume that self implements the SpeechEventListener and PipelineDelegate protocols</span>
<span class="k">let</span> <span class="nv">pipeline</span> <span class="o">=</span> <span class="kt">SpeechPipeline</span><span class="p">(</span><span class="kt">SpeechProcessors</span><span class="o">.</span><span class="n">appleSpeech</span><span class="o">.</span><span class="n">processor</span><span class="p">,</span>
                               <span class="nv">speechConfiguration</span><span class="p">:</span> <span class="kt">SpeechConfiguration</span><span class="p">(),</span>
                               <span class="nv">speechDelegate</span><span class="p">:</span> <span class="k">self</span><span class="p">,</span>
                               <span class="nv">wakewordService</span><span class="p">:</span> <span class="kt">SpeechProcessors</span><span class="o">.</span><span class="n">appleWakeword</span><span class="o">.</span><span class="n">processor</span><span class="p">,</span>
                               <span class="nv">pipelineDelegate</span><span class="p">:</span> <span class="k">self</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="nf">start</span><span class="p">()</span>
</code></pre>
<div class="aside aside-warning">
    <p class="aside-title">Warning</p>
    All calls to delegate event handlers are made in the context of the pipeline&rsquo;s thread, so event handlers should not perform blocking operations, and should use message passing when communicating with UI components, etc.

</div>

                        <a href="Classes/SpeechPipeline.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">@objc</span>
<span class="kd">public</span> <span class="kd">final</span> <span class="kd">class</span> <span class="kt">SpeechPipeline</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer"></a>
                    <a name="//apple_ref/swift/Class/TFLiteWakewordRecognizer" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer">TFLiteWakewordRecognizer</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>This pipeline component streams audio samples and uses a Tensorflow-Lite binary classifier to detect keyword phrases to process for wakeword recognition.  Once a wakeword phrase is detected, the speech pipeline is activated. The speech pipeline remains active until the user stops talking or the activation timeout is reached.</p>

<p>When the speech pipeline coordination event is started via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer begins streaming buffered frames  that are first normalized and then converted to the magnitude Short-Time Fourier Transform (STFT) representation over a hopped sliding window. This linear spectrogram is then converted to a mel spectrogram via a <q>filter</q> Tensorflow model. These mel frames are batched together into a sliding window.</p>

<p>The mel spectrogram represents the features to be passed to the autoregressive encoder (usually an rnn or crnn), which is implemented in an <q>encode</q> Tensorflow model. This encoder outputs an encoded vector and a state vector. The encoded vectors are batched together into a sliding window for classification, and the state vector is used to perform the running autoregressive transduction over the mel frames.</p>

<p>The <q>detect</q> Tensorflow model takes the encoded sliding window and outputs a single posterior value in the range [0, 1]. Values closer to 1 indicate a detected keyword phrase, values closer to 0 indicate non-keyword speech. This classifier is commonly implemented as an attention mechanism over the encoder window.</p>

<p>The detector&rsquo;s outputs are then compared against a configured threshold, in order to determine whether to activate the pipeline. If the posterior is greater than the thresold, the activation occurs.</p>

<p>Upon the pipeline activation event, the recognizer completes processing and awaits another coordination event. When the speech pipeline coordination is stopped via the <code><a href="Protocols/SpeechProcessor.html">SpeechProcessor</a></code> protocol implementation, the recognizer stops processing and awaits another coordination event.</p>

                        <a href="Classes/TFLiteWakewordRecognizer.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">public</span> <span class="kd">class</span> <span class="kt">TFLiteWakewordRecognizer</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
            <div class="task-group">
              <ul>
                <li class="item">
                  <div>
                    <code>
                    <a name="/c:@M@Spokestack_iOS@objc(cs)WebRTCVAD"></a>
                    <a name="//apple_ref/swift/Class/WebRTCVAD" class="dashAnchor"></a>
                    <a class="token" href="#/c:@M@Spokestack_iOS@objc(cs)WebRTCVAD">WebRTCVAD</a>
                    </code>
                  </div>
                  <div class="height-container">
                    <div class="pointer-container"></div>
                    <section class="section">
                      <div class="pointer"></div>
                      <div class="abstract">
                        <p>Swift wrapper for WebRTC&rsquo;s voice activity detector.</p>

                        <a href="Classes/WebRTCVAD.html" class="slightly-smaller">See more</a>
                      </div>
                      <div class="declaration">
                        <h4>Declaration</h4>
                        <div class="language">
                          <p class="aside-title">Swift</p>
                          <pre class="highlight swift"><code><span class="kd">public</span> <span class="kd">class</span> <span class="kt">WebRTCVAD</span> <span class="p">:</span> <span class="kt">NSObject</span></code></pre>

                        </div>
                      </div>
                    </section>
                  </div>
                </li>
              </ul>
            </div>
          </section>
        </section>
        <section id="footer">
          <p>&copy; 2019 <a class="link" href="https://www.spokestack.io" target="_blank" rel="external">Spokestack</a>. All rights reserved. (Last updated: 2019-11-08)</p>
          <p>Generated by <a class="link" href="https://github.com/realm/jazzy" target="_blank" rel="external">jazzy ♪♫ v0.11.2</a>, a <a class="link" href="https://realm.io" target="_blank" rel="external">Realm</a> project.</p>
        </section>
      </article>
    </div>
  </body>
</div>
</html>
