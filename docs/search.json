{"Typealiases.html#/s:14Spokestack_iOS24SpeechRecognitionClosurea":{"name":"SpeechRecognitionClosure","abstract":"<p>Undocumented</p>"},"Structs/Trace/Level.html#/s:14Spokestack_iOS5TraceV5LevelO4NONEyA2EmF":{"name":"NONE","abstract":"<p>No traces</p>","parent_name":"Level"},"Structs/Trace/Level.html#/s:14Spokestack_iOS5TraceV5LevelO4INFOyA2EmF":{"name":"INFO","abstract":"<p>Informational traces</p>","parent_name":"Level"},"Structs/Trace/Level.html#/s:14Spokestack_iOS5TraceV5LevelO4PERFyA2EmF":{"name":"PERF","abstract":"<p>Performance traces</p>","parent_name":"Level"},"Structs/Trace/Level.html#/s:14Spokestack_iOS5TraceV5LevelO5DEBUGyA2EmF":{"name":"DEBUG","abstract":"<p>All the traces</p>","parent_name":"Level"},"Structs/Trace/Level.html":{"name":"Level","abstract":"<p>Undocumented</p>","parent_name":"Trace"},"Structs/Trace.html#/s:14Spokestack_iOS5TraceV5trace_11configLevel7message8delegate6calleryAC0F0O_AJSSAA19SpeechEventListener_pSgyptFZ":{"name":"trace(_:configLevel:message:delegate:caller:)","abstract":"<p>Traces a  debugging message.</p>","parent_name":"Trace"},"Structs/Trace.html#/s:14Spokestack_iOS5TraceV5trace_11configLevel7message8delegate6calleryAC0F0O_AJSSAA16PipelineDelegate_pSgyptFZ":{"name":"trace(_:configLevel:message:delegate:caller:)","abstract":"<p>Traces a debugging message.</p>","parent_name":"Trace"},"Structs/Trace.html#/s:14Spokestack_iOS5TraceV4spit4data8fileName8delegatey10Foundation4DataV_SSAA19SpeechEventListener_ptFZ":{"name":"spit(data:fileName:delegate:)","abstract":"<p>Write data to a file, after clojure/core&rsquo;s <code>spit</code>.</p>","parent_name":"Trace"},"Structs/SignalProcessing/FFTWindowType.html#/s:14Spokestack_iOS16SignalProcessingV13FFTWindowTypeO4hannyA2EmF":{"name":"hann","abstract":"<p>Undocumented</p>","parent_name":"FFTWindowType"},"Structs/SignalProcessing.html#/s:14Spokestack_iOS16SignalProcessingV3rmsySf10Foundation4DataV_Says5Int16VGtFZ":{"name":"rms(_:_:)","abstract":"<p>Find the root mean squared of a frame buffer of samples.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing/FFTWindowType.html":{"name":"FFTWindowType","abstract":"<p>Convenience enum for Fast Fourier Transform window types.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing.html#/s:14Spokestack_iOS16SignalProcessingV17fftWindowDispatch10windowType0H6LengthSaySfGAC09FFTWindowI0O_SitFZ":{"name":"fftWindowDispatch(windowType:windowLength:)","abstract":"<p>Convenience function to find the window of a FFT.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing.html#/s:14Spokestack_iOS16SignalProcessingV10hannWindowySaySfGSiFZ":{"name":"hannWindow(_:)","abstract":"<p>Implementation of the Hann smoothing function algorithm.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing.html":{"name":"SignalProcessing","abstract":"<p>Static namepsace for signal processing functions.</p>"},"Structs/Trace.html":{"name":"Trace","abstract":"<p>Debugging trace levels, for simple filtering.</p>"},"Protocols/VADDelegate.html#/c:@M@Spokestack_iOS@objc(pl)VADDelegate(im)activateWithFrame:":{"name":"activate(frame:)","abstract":"<p>The VAD has detected speech.</p>","parent_name":"VADDelegate"},"Protocols/VADDelegate.html#/c:@M@Spokestack_iOS@objc(pl)VADDelegate(im)deactivate":{"name":"deactivate()","abstract":"<p>The VAD has stopped detecting speech.</p>","parent_name":"VADDelegate"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack_iOS@objc(pl)SpeechProcessor(py)configuration":{"name":"configuration","abstract":"<p>The global configuration for all speech pipeline components.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack_iOS@objc(pl)SpeechProcessor(py)delegate":{"name":"delegate","abstract":"<p>Delegate for sending speech pipeline control events.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack_iOS@objc(pl)SpeechProcessor(py)context":{"name":"context","abstract":"<p>Global speech context.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack_iOS@objc(pl)SpeechProcessor(im)startStreamingWithContext:":{"name":"startStreaming(context:)","abstract":"<p>Trigger from the speech pipeline for the component to begin processing the audio stream.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack_iOS@objc(pl)SpeechProcessor(im)stopStreamingWithContext:":{"name":"stopStreaming(context:)","abstract":"<p>Trigger from the speech pipeline for the component to stop processing the audio stream.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)activate":{"name":"activate()","abstract":"<p>The pipeline activate event. Occurs upon activation of speech recognition.  The pipeline remains active until the user stops talking or the activation timeout is reached.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)deactivate":{"name":"deactivate()","abstract":"<p>The pipeline deactivate event. Occurs upon deactivation of speech recognition.  The pipeline remains inactive until activated again by either explicit activation or wakeword activation.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)didError:":{"name":"didError(_:)","abstract":"<p>The error event. An error occured in the speech pipeline.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)didTrace:":{"name":"didTrace(_:)","abstract":"<p>The debug trace event.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)didStop":{"name":"didStop()","abstract":"<p>The pipeline stop event. The pipeline was stopped.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)didStart":{"name":"didStart()","abstract":"<p>The pipeline start event. The pipeline was started and is in the deactivated state.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)didRecognize:":{"name":"didRecognize(_:)","abstract":"<p>The pipeline speech recognition result event. The pipeline was activated and recognized speech.</p>","parent_name":"SpeechEventListener"},"Protocols/SpeechEventListener.html#/c:@M@Spokestack_iOS@objc(pl)SpeechEventListener(im)didTimeout":{"name":"didTimeout()","abstract":"<p>The pipeline timeout event. The pipeline experienced a timeout in a component.</p>","parent_name":"SpeechEventListener"},"Protocols/PipelineDelegate.html#/c:@M@Spokestack_iOS@objc(pl)PipelineDelegate(im)didInit":{"name":"didInit()","abstract":"<p>The speech pipeline has been initialized.</p>","parent_name":"PipelineDelegate"},"Protocols/PipelineDelegate.html#/c:@M@Spokestack_iOS@objc(pl)PipelineDelegate(im)didStart":{"name":"didStart()","abstract":"<p>The speech pipeline has been started.</p>","parent_name":"PipelineDelegate"},"Protocols/PipelineDelegate.html#/c:@M@Spokestack_iOS@objc(pl)PipelineDelegate(im)didStop":{"name":"didStop()","abstract":"<p>The speech pipeline has been stopped.</p>","parent_name":"PipelineDelegate"},"Protocols/PipelineDelegate.html#/c:@M@Spokestack_iOS@objc(pl)PipelineDelegate(im)setupFailed:":{"name":"setupFailed(_:)","abstract":"<p>The speech pipeline encountered an error during initialization.</p>","parent_name":"PipelineDelegate"},"Protocols/PipelineDelegate.html#/c:@M@Spokestack_iOS@objc(pl)PipelineDelegate(im)didTrace:":{"name":"didTrace(_:)","abstract":"<p>A trace event from the speech pipeline.</p>","parent_name":"PipelineDelegate"},"Protocols/PipelineDelegate.html":{"name":"PipelineDelegate","abstract":"<p>Protocol for delegates to receive pipeline events.</p>"},"Protocols/SpeechEventListener.html":{"name":"SpeechEventListener","abstract":"<p>Functions required for components to receive speech pipeline control events.</p>"},"Protocols/SpeechProcessor.html":{"name":"SpeechProcessor","abstract":"<p>Protocol for speech pipeline components to receive speech pipeline coordination events.</p>"},"Protocols/VADDelegate.html":{"name":"VADDelegate","abstract":"<p>Protocol for receiving VAD activation and error events.</p>"},"Enums/VADMode.html#/s:14Spokestack_iOS7VADModeO16HighlyPermissiveyA2CmF":{"name":"HighlyPermissive","abstract":"<p>Most permissive of non-speech; least likely to not detection speech; most likely to detect speech.</p>","parent_name":"VADMode"},"Enums/VADMode.html#/s:14Spokestack_iOS7VADModeO10PermissiveyA2CmF":{"name":"Permissive","abstract":"<p>Allows more non-speech than .higher levels.</p>","parent_name":"VADMode"},"Enums/VADMode.html#/s:14Spokestack_iOS7VADModeO11RestrictiveyA2CmF":{"name":"Restrictive","abstract":"<p>Allows less non-speech than higher levels.</p>","parent_name":"VADMode"},"Enums/VADMode.html#/s:14Spokestack_iOS7VADModeO17HighlyRestrictiveyA2CmF":{"name":"HighlyRestrictive","abstract":"<p>Most restrictive of non-speech; most amount of miss speech.</p>","parent_name":"VADMode"},"Enums/SpeechProcessors.html#/c:@M@Spokestack_iOS@E@SpeechProcessors@SpeechProcessorsAppleWakeword":{"name":"appleWakeword","abstract":"<p>AppleWakewordRecognizer</p>","parent_name":"SpeechProcessors"},"Enums/SpeechProcessors.html#/c:@M@Spokestack_iOS@E@SpeechProcessors@SpeechProcessorsCoremlWakeword":{"name":"coremlWakeword","abstract":"<p>CoreMLWakewordRecognizer</p>","parent_name":"SpeechProcessors"},"Enums/SpeechProcessors.html#/c:@M@Spokestack_iOS@E@SpeechProcessors@SpeechProcessorsTfLiteWakeword":{"name":"tfLiteWakeword","abstract":"<p>TFLiteWakewordRecognizer</p>","parent_name":"SpeechProcessors"},"Enums/SpeechProcessors.html#/c:@M@Spokestack_iOS@E@SpeechProcessors@SpeechProcessorsAppleSpeech":{"name":"appleSpeech","abstract":"<p>AppleSpeechRecognizer</p>","parent_name":"SpeechProcessors"},"Enums/SpeechProcessors.html#/s:14Spokestack_iOS16SpeechProcessorsO9processorAA0C9Processor_pvp":{"name":"processor","abstract":"<p>Convenience property accessor for the singletons of the different implementers of the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpeechProcessor.html\">SpeechProcessor</a></code> protocol</p>","parent_name":"SpeechProcessors"},"Enums/SPSpeechRecognitionResult.html#/s:14Spokestack_iOS25SPSpeechRecognitionResultO7successyACyxq_GxcAEms5ErrorR_r0_lF":{"name":"success(_:)","abstract":"<p>Undocumented</p>","parent_name":"SPSpeechRecognitionResult"},"Enums/SPSpeechRecognitionResult.html#/s:14Spokestack_iOS25SPSpeechRecognitionResultO7failureyACyxq_Gq_cAEms5ErrorR_r0_lF":{"name":"failure(_:)","abstract":"<p>Undocumented</p>","parent_name":"SPSpeechRecognitionResult"},"Enums/SPSpeechRecognitionError.html#/s:14Spokestack_iOS24SPSpeechRecognitionErrorO7generalyACSScACmF":{"name":"general(_:)","abstract":"<p>Undocumented</p>","parent_name":"SPSpeechRecognitionError"},"Enums/SPSpeechRecognitionError.html#/s:14Spokestack_iOS24SPSpeechRecognitionErrorO11emptyresultyACSScACmF":{"name":"emptyresult(_:)","abstract":"<p>Undocumented</p>","parent_name":"SPSpeechRecognitionError"},"Enums/WakewordModelError.html#/s:14Spokestack_iOS18WakewordModelErrorO5modelyACSScACmF":{"name":"model(_:)","abstract":"<p>The WakewordRecognizer was unable to configure the recognizer model(s).</p>","parent_name":"WakewordModelError"},"Enums/WakewordModelError.html#/s:14Spokestack_iOS18WakewordModelErrorO7processyACSScACmF":{"name":"process(_:)","abstract":"<p>The WakewordRecognizer encountered an error during the processing of the audio frame.</p>","parent_name":"WakewordModelError"},"Enums/WakewordModelError.html#/s:14Spokestack_iOS18WakewordModelErrorO6filteryACSScACmF":{"name":"filter(_:)","abstract":"<p>The WakewordRecognizer encountered an error during the configuration or running of the filter model.</p>","parent_name":"WakewordModelError"},"Enums/WakewordModelError.html#/s:14Spokestack_iOS18WakewordModelErrorO6encodeyACSScACmF":{"name":"encode(_:)","abstract":"<p>The WakewordRecognizer encountered an error during the configuration or running of the encode model.</p>","parent_name":"WakewordModelError"},"Enums/WakewordModelError.html#/s:14Spokestack_iOS18WakewordModelErrorO6detectyACSScACmF":{"name":"detect(_:)","abstract":"<p>The WakewordRecognizer encountered an error during the configuration or running of the detect model.</p>","parent_name":"WakewordModelError"},"Enums/VADError.html#/s:14Spokestack_iOS8VADErrorO20invalidConfigurationyACSScACmF":{"name":"invalidConfiguration(_:)","abstract":"<p>The VAD instance was configured with incompatible settings.</p>","parent_name":"VADError"},"Enums/VADError.html#/s:14Spokestack_iOS8VADErrorO14initializationyACSScACmF":{"name":"initialization(_:)","abstract":"<p>The VAD instance was unable to initialize.</p>","parent_name":"VADError"},"Enums/VADError.html#/s:14Spokestack_iOS8VADErrorO10processingyACSScACmF":{"name":"processing(_:)","abstract":"<p>The VAD instance encountered an error during the processing of the audio frame.</p>","parent_name":"VADError"},"Enums/SpeechPipelineError.html#/s:14Spokestack_iOS19SpeechPipelineErrorO12illegalStateyACSScACmF":{"name":"illegalState(_:)","abstract":"<p>The SpeechPipeline internal buffers entered an illegal state.</p>","parent_name":"SpeechPipelineError"},"Enums/AudioError.html#/s:14Spokestack_iOS10AudioErrorO17audioSessionSetupyACSScACmF":{"name":"audioSessionSetup(_:)","abstract":"<p>An audio unit system error</p>","parent_name":"AudioError"},"Enums/AudioError.html":{"name":"AudioError","abstract":"<p>Errors thrown by <code>AudioController</code> during <code>startStreaming</code> and <code>stopStreaming</code>.</p>"},"Enums/SpeechPipelineError.html":{"name":"SpeechPipelineError","abstract":"<p>Errors thrown by the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechPipeline.html\">SpeechPipeline</a></code>.</p>"},"Enums/VADError.html":{"name":"VADError","abstract":"<p>Errors thrown by the Voice Activity Detector.</p>"},"Enums/WakewordModelError.html":{"name":"WakewordModelError","abstract":"<p>Errors thrown by implementors of the WakewordRecognizer protocol.</p>"},"Enums/SPSpeechRecognitionError.html":{"name":"SPSpeechRecognitionError","abstract":"<p>CloudKit</p>"},"Enums/SPSpeechRecognitionResult.html":{"name":"SPSpeechRecognitionResult","abstract":"<p>Undocumented</p>"},"Enums/SpeechProcessors.html":{"name":"SpeechProcessors","abstract":"<p>Convenience enum for the singletons of the different implementers of the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpeechProcessor.html\">SpeechProcessor</a></code> protocol.</p>"},"Enums/VADMode.html":{"name":"VADMode","abstract":"<p>Indicate how likely it is that non-speech will activate the VAD.</p>"},"Classes/WebRTCVAD.html#/s:14Spokestack_iOS9WebRTCVADC8delegateAA11VADDelegate_pSgvp":{"name":"delegate","abstract":"<p>Callback delegate for activation and error events.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/s:14Spokestack_iOS9WebRTCVADC6create4mode8delegate10frameWidth10sampleRateyAA7VADModeO_AA11VADDelegate_pS2itKF":{"name":"create(mode:delegate:frameWidth:sampleRate:)","abstract":"<p>Creates and configures a new WebRTC VAD component.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/s:14Spokestack_iOS9WebRTCVADC7process5frame8isSpeechy10Foundation4DataV_SbtKF":{"name":"process(frame:isSpeech:)","abstract":"<p>Processes an audio frame, detecting speech.</p>","parent_name":"WebRTCVAD"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(cpy)sharedInstance":{"name":"sharedInstance","abstract":"<p>Singleton instance.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(py)delegate":{"name":"delegate","abstract":"<p>Delegate which receives speech pipeline control events.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(im)init":{"name":"init()","abstract":"<p>Undocumented</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(im)startStreamingWithContext:":{"name":"startStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(im)stopStreamingWithContext:":{"name":"stopStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(im)activateWithFrame:":{"name":"activate(frame:)","abstract":"<p>Called when the VAD has detected speech.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)TFLiteWakewordRecognizer(im)deactivate":{"name":"deactivate()","abstract":"<p>Called when the VAD as stopped detecting speech.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/SpeechPipeline.html#/s:14Spokestack_iOS14SpeechPipelineC19speechConfigurationAA0cF0CSgvp":{"name":"speechConfiguration","abstract":"<p>Pipeline configuration parameters.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/s:14Spokestack_iOS14SpeechPipelineC14speechDelegateAA0C13EventListener_pSgvp":{"name":"speechDelegate","abstract":"<p>Delegate that receives speech events.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/s:14Spokestack_iOS14SpeechPipelineC16pipelineDelegateAA0dF0_pSgvp":{"name":"pipelineDelegate","abstract":"<p>Delegate that receives pipeline events.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/s:14Spokestack_iOS14SpeechPipelineC7contextAA0C7ContextCvp":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)init:speechConfiguration:speechDelegate:wakewordService:pipelineDelegate:error:":{"name":"init(_:speechConfiguration:speechDelegate:wakewordService:pipelineDelegate:)","abstract":"<p>Initializes a new speech pipeline instance.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)status":{"name":"status()","abstract":"<p>Checks the status of the delegates provided in the constructor.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)setDelegates:":{"name":"setDelegates(_:)","abstract":"<p>Sets the property for the<code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpeechEventListener.html\">SpeechEventListener</a></code> delegate .</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)activate":{"name":"activate()","abstract":"<p>Activates speech recognition. The pipeline remains active until the user stops talking or the activation timeout is reached.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)deactivate":{"name":"deactivate()","abstract":"<p>Deactivates speech recognition.  The pipeline returns to awaiting either wakeword activation or an explicit <code>activate</code> call.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)start":{"name":"start()","abstract":"<p>Starts  the speech pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack_iOS@objc(cs)SpeechPipeline(im)stop":{"name":"stop()","abstract":"<p>Stops the speech pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechContext.html#/c:@M@Spokestack_iOS@objc(cs)SpeechContext(py)transcript":{"name":"transcript","abstract":"<p>Current speech transcript</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack_iOS@objc(cs)SpeechContext(py)confidence":{"name":"confidence","abstract":"<p>Current speech recognition confidence: [0-1)</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack_iOS@objc(cs)SpeechContext(py)isStarted":{"name":"isStarted","abstract":"<p>Speech pipeline active indicator</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack_iOS@objc(cs)SpeechContext(py)isActive":{"name":"isActive","abstract":"<p>Speech recognition active indicator</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack_iOS@objc(cs)SpeechContext(py)isSpeech":{"name":"isSpeech","abstract":"<p>Speech detected indicator</p>","parent_name":"SpeechContext"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)wakewords":{"name":"wakewords","abstract":"<p>A comma-separated list of wakeword keywords, in the order they appear in the classifier outputs, not including the null (non-keyword) class.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)wakePhrases":{"name":"wakePhrases","abstract":"<p>A comma-separated list of space-separated wakeword keyword phrases to detect, which defaults to no phrases (just individual keywords).</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:14Spokestack_iOS19SpeechConfigurationC16wakePhraseLengthSivp":{"name":"wakePhraseLength","abstract":"<p>The length of the wakeword phraser&rsquo;s sliding window, in milliseconds - this value should be long enough to fit the longest supported phrase.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:14Spokestack_iOS19SpeechConfigurationC16wakeSmoothLengthSivp":{"name":"wakeSmoothLength","abstract":"<p>The length of the posterior smoothing window to use with the wakeword classifier&rsquo;s outputs, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:14Spokestack_iOS19SpeechConfigurationC13fftWindowTypeAA16SignalProcessingV09FFTWindowG0Ovp":{"name":"fftWindowType","abstract":"<p>The name of the window function to apply to each audio frame before calculating the STFT.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)rmsTarget":{"name":"rmsTarget","abstract":"<p>The desired linear Root Mean Squared (RMS) signal energy, which is used for signal normalization and should be tuned to the RMS target used during wakeword model training.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)rmsAlpha":{"name":"rmsAlpha","abstract":"<p>The Exponentially Weighted Moving Average (EWMA) update rate for the current  Root Mean Squared (RMS) signal energy (0 for no RMS normalization).</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)fftWindowSize":{"name":"fftWindowSize","abstract":"<p>The size of the signal window used to calculate the STFT, in number of samples - should be a power of 2 for maximum efficiency.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)fftHopLength":{"name":"fftHopLength","abstract":"<p>The length of time to skip each time the overlapping STFT is calculated, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)melFrameLength":{"name":"melFrameLength","abstract":"<p>The length of a frame in the mel spectrogram used as an input to the wakeword recognizer encoder, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)melFrameWidth":{"name":"melFrameWidth","abstract":"<p>The number of filterbank components in each mel spectrogram frame sent to the wakeword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)stateWidth":{"name":"stateWidth","abstract":"<p>The size of the wakeword recognizer&rsquo;s encoder state output.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)encodeWidth":{"name":"encodeWidth","abstract":"<p>The size of the wakeword recognizer&rsquo;s encoder window output.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)encodeLength":{"name":"encodeLength","abstract":"<p>The length of the sliding window of encoder output used as an input to the wakeword recognizer classifier, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)wakeThreshold":{"name":"wakeThreshold","abstract":"<p>The threshold of the wakeword recognizer classifier&rsquo;s posterior output, above which the wakeword recognizer activates the pipeline, in the range [0, 1].</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)wakeActiveMin":{"name":"wakeActiveMin","abstract":"<p>The minimum length of an activation, in milliseconds. Used to ignore a Voice Activity Detector (VAD) deactivation after the wakeword.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)wakeActiveMax":{"name":"wakeActiveMax","abstract":"<p>The maximum length of an activation, in milliseconds. Used to time out the speech pipeline activation.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:14Spokestack_iOS19SpeechConfigurationC7vadModeAA7VADModeOvp":{"name":"vadMode","abstract":"<p>Indicate to the VAD the level of permissiveness to non-speech activation.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)vadFallDelay":{"name":"vadFallDelay","abstract":"<p>Delay between a VAD deactivation and the delivery of the recognition results.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:14Spokestack_iOS19SpeechConfigurationC10sampleRateSivp":{"name":"sampleRate","abstract":"<p>Audio sampling rate, in Hz.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)frameWidth":{"name":"frameWidth","abstract":"<p>Audio frame width, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)wakewordRequestTimeout":{"name":"wakewordRequestTimeout","abstract":"<p>Length of time to allow an Apple ASR request to run, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)preEmphasis":{"name":"preEmphasis","abstract":"<p>The pre-emphasis filter weight to apply to the normalized audio signal, in a range of [0, 1].</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)filterModelName":{"name":"filterModelName","abstract":"<p>The filename of the machine learning model used for the filtering step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)encodeModelName":{"name":"encodeModelName","abstract":"<p>The filename of the machine learning model used for the encoding step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)detectModelName":{"name":"detectModelName","abstract":"<p>The filename of the machine learning model used for the detect step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)filterModelPath":{"name":"filterModelPath","abstract":"<p>The filesystem path to the machine learning model for the filtering step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)encodeModelPath":{"name":"encodeModelPath","abstract":"<p>The filesystem path to the machine learning model for the encoding step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)detectModelPath":{"name":"detectModelPath","abstract":"<p>The filesystem path to the machine learning model for the detect step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack_iOS@objc(cs)SpeechConfiguration(py)tracing":{"name":"tracing","abstract":"<p>Debugging trace levels, for simple filtering.</p>","parent_name":"SpeechConfiguration"},"Classes/CoreMLWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(cpy)sharedInstance":{"name":"sharedInstance","abstract":"<p>Singleton instance.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(py)delegate":{"name":"delegate","abstract":"<p>Delegate which receives speech pipeline control events.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(im)init":{"name":"init()","abstract":"<p>Undocumented</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(im)startStreamingWithContext:":{"name":"startStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(im)stopStreamingWithContext:":{"name":"stopStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(im)activateWithFrame:":{"name":"activate(frame:)","abstract":"<p>Called when the VAD has detected speech.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/CoreMLWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)CoreMLWakewordRecognizer(im)deactivate":{"name":"deactivate()","abstract":"<p>Called when the VAD has stopped detecting speech.</p>","parent_name":"CoreMLWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(cpy)sharedInstance":{"name":"sharedInstance","abstract":"<p>Singleton instance.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(py)delegate":{"name":"delegate","abstract":"<p>Delegate which receives speech pipeline control events.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(im)init":{"name":"init()","abstract":"<p>Undocumented</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(im)startStreamingWithContext:":{"name":"startStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(im)stopStreamingWithContext:":{"name":"stopStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(im)activateWithFrame:":{"name":"activate(frame:)","abstract":"<p>Called when the VAD has detected speech.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack_iOS@objc(cs)AppleWakewordRecognizer(im)deactivate":{"name":"deactivate()","abstract":"<p>Called when the VAD has stopped detecting speech.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer(cpy)sharedInstance":{"name":"sharedInstance","abstract":"<p>Singleton instance.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer(py)delegate":{"name":"delegate","abstract":"<p>Delegate which receives speech pipeline control events.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer(im)startStreamingWithContext:":{"name":"startStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack_iOS@objc(cs)AppleSpeechRecognizer(im)stopStreamingWithContext:":{"name":"stopStreaming(context:)","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html":{"name":"AppleSpeechRecognizer","abstract":"<p>This pipeline component uses the Apple <code>SFSpeech</code> API to stream audio samples for speech recognition.</p>"},"Classes/AppleWakewordRecognizer.html":{"name":"AppleWakewordRecognizer","abstract":"<p>This pipeline component uses the Apple <code>SFSpeech</code> API to stream audio samples for wakeword recognition.</p>"},"Classes/CoreMLWakewordRecognizer.html":{"name":"CoreMLWakewordRecognizer","abstract":"<p>This pipeline component streams audio samples and uses CoreML models to detect and aggregate keywords (i.e. [null], up, dog) into phrases (up dog) to process for wakeword recognition. Once a wakeword phrase is detected, speech pipeline activation is called. </p>"},"Classes/SpeechConfiguration.html":{"name":"SpeechConfiguration","abstract":"<p>Configuration properties for the pipeline abstraction to pass down to implementations.</p>"},"Classes/SpeechContext.html":{"name":"SpeechContext","abstract":"<p>This class maintains global state for the speech pipeline, allowing pipeline components to communicate information among themselves and event handlers.</p>"},"Classes/SpeechPipeline.html":{"name":"SpeechPipeline","abstract":"<p>This is the primary client entry point to the SpokeStack framework. It dynamically binds to configured components that implement the pipeline interfaces for reading audio frames and performing speech recognition tasks.</p>"},"Classes/TFLiteWakewordRecognizer.html":{"name":"TFLiteWakewordRecognizer","abstract":"<p>This pipeline component streams audio samples and uses a TensorFlow Lite binary classifier to detect keyword phrases to process for wakeword recognition. Once a wakeword phrase is detected, the speech pipeline is activated.</p>"},"Classes/WebRTCVAD.html":{"name":"WebRTCVAD","abstract":"<p>Swift wrapper for WebRTC&rsquo;s voice activity detector.</p>"},"Classes.html":{"name":"Classes","abstract":"<p>The following classes are available globally.</p>"},"Enums.html":{"name":"Enumerations","abstract":"<p>The following enumerations are available globally.</p>"},"Protocols.html":{"name":"Protocols","abstract":"<p>The following protocols are available globally.</p>"},"Structs.html":{"name":"Structures","abstract":"<p>The following structures are available globally.</p>"},"Typealiases.html":{"name":"Type Aliases","abstract":"<p>The following type aliases are available globally.</p>"}}